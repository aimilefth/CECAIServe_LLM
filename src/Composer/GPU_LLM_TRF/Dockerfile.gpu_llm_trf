FROM aimilefth/cecaiserve_base_images:gpu_llm_trf 

# Install any packages in the requirements file
ARG REQUIREMENTS_FILE_ARG=extra_pip_libraries_gpu_llm_trf.txt
COPY ${REQUIREMENTS_FILE_ARG} ./
RUN if [ -s ./${REQUIREMENTS_FILE_ARG} ]; then /usr/bin/python3 -m pip install -r ./${REQUIREMENTS_FILE_ARG}; fi
# Remove the requirements file from the image
RUN rm -f ./${REQUIREMENTS_FILE_ARG}

# Define arguments (Constants)
ARG WORKING_DIR_ARG=/home/Documents
ARG API_APP_ARG=api_server.py
ARG UTILS_APP_ARG=utils.py
ARG WORKFLOW_SERVER_APP_ARG=workflow_server.py
ARG MODEL_SERVER_APP_ARG=model_server.py
ARG GPU_LLM_TRF_SERVER_APP_ARG=gpu_llm_trf_server.py
ARG MY_SERVER_APP_ARG=my_server.py
ARG POWER_SCRAPER_APP_ARG=power_scraper.py
ARG ENV_FILE_ARG=.env
ARG LOG_CONFIG_ARG=logconfig.ini
ARG EXTRA_FILES_DIR_ARG=extra_files_dir

# (docker_build_args)
ARG SERVER_IP_ARG=0.0.0.0
ARG SERVER_PORT_ARG=3000
ARG MODEL_NAME_ARG
ARG APP_NAME_ARG
ARG NETWORK_NAME_ARG
ARG SERVER_MODE_ARG
ARG BATCH_SIZE_ARG
ARG PRECISION_ARG=FP32

# Generation knobs
ARG MAX_LENGTH_ARG
ARG DO_SAMPLE_ARG
ARG TEMPERATURE_ARG
ARG TOP_P_ARG

# Convert arguments to environmental variables
ENV API_APP=${API_APP_ARG}
ENV WORKFLOW_SERVER_APP=${WORKFLOW_SERVER_APP_ARG}
ENV MODEL_SERVER_APP=${MODEL_SERVER_APP_ARG}
ENV GPU_LLM_TRF_SERVER_APP=${GPU_LLM_TRF_SERVER_APP_ARG}
ENV MY_SERVER_APP=${MY_SERVER_APP_ARG}
ENV ENV_FILE=${ENV_FILE_ARG}
ENV MODEL_NAME=${MODEL_NAME_ARG}
ENV LOG_CONFIG=${LOG_CONFIG_ARG}
ENV SERVER_IP=${SERVER_IP_ARG}
ENV SERVER_PORT=${SERVER_PORT_ARG}

ENV LOG_FILE=AIF_template_GPU_LLM_TRF.log
ENV SEND_METRICS=False
ENV METRICS_LIST_SIZE=1000
ENV DEBUG_MODE=False

ENV APP_NAME=${APP_NAME_ARG}
ENV NETWORK_NAME=${NETWORK_NAME_ARG}
ENV AI_DEVICE=GPU_LLM_TRF
ENV SERVER_MODE=${SERVER_MODE_ARG}
ENV NODE_NAME=Unknown
ENV BATCH_SIZE=${BATCH_SIZE_ARG}
ENV PRECISION=${PRECISION_ARG}

ENV MAX_LENGTH=${MAX_LENGTH_ARG}
ENV DO_SAMPLE=${DO_SAMPLE_ARG}
ENV TEMPERATURE=${TEMPERATURE_ARG}
ENV TOP_P=${TOP_P_ARG}

ENV GPU_DEVICE_INDEX=1

# Set this to match the correct NVIDIA GPU ORDERING. Solves this issue https://github.com/LostRuins/koboldcpp/issues/1023
ENV CUDA_DEVICE_ORDER=PCI_BUS_ID

# Copy files from the local filesystem to the working directory in the Docker image
RUN mkdir -p ${WORKING_DIR_ARG}
COPY ${MODEL_NAME_ARG} ${WORKING_DIR_ARG}/${MODEL_NAME_ARG}
COPY ${API_APP_ARG} ${WORKING_DIR_ARG}
COPY ${WORKFLOW_SERVER_APP_ARG} ${WORKING_DIR_ARG}
COPY ${MODEL_SERVER_APP_ARG} ${WORKING_DIR_ARG}
COPY ${GPU_LLM_TRF_SERVER_APP_ARG} ${WORKING_DIR_ARG} 
COPY ${MY_SERVER_APP_ARG} ${WORKING_DIR_ARG} 
COPY ${POWER_SCRAPER_APP_ARG} ${WORKING_DIR_ARG}
COPY ${LOG_CONFIG_ARG} ${WORKING_DIR_ARG}
COPY ${UTILS_APP_ARG} ${WORKING_DIR_ARG}
COPY ${ENV_FILE_ARG} ${WORKING_DIR_ARG}
# Optional Copy if ${EXTRA_FILES_DIR_ARG} directory exists
COPY ${EXTRA_FILES_DIR_ARG}*/* ${WORKING_DIR_ARG}
WORKDIR ${WORKING_DIR_ARG}

# Expose the server port
EXPOSE ${SERVER_PORT_ARG}

# The command to run when the container is started
CMD python3 ${API_APP}